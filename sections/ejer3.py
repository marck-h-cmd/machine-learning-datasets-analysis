import streamlit as st
import pandas as pd
import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns
from io import StringIO

def main():
    st.header("Ejercicio 3: Dataset 'Iris'")
    
    st.markdown("""
    ### Objetivo
    Implementar un flujo completo de preprocesamiento y visualizar resultados.
    """)
    
    # Cargar dataset desde sklearn
    st.subheader("1Ô∏è‚É£ Carga del Dataset desde scikit-learn")
    
    iris = load_iris()
    df = pd.DataFrame(iris.data, columns=iris.feature_names)
    df['target'] = iris.target
    df['target_name'] = df['target'].apply(lambda x: iris.target_names[x])
    
    st.success("‚úÖ Dataset Iris cargado desde sklearn.datasets")
    
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"**Dimensiones:** {df.shape[0]} filas √ó {df.shape[1]} columnas")
        st.write(f"**Caracter√≠sticas:** {len(iris.feature_names)}")
        st.write(f"**Clases:** {len(iris.target_names)}")
    with col2:
        st.write("**Nombres de caracter√≠sticas:**")
        for i, name in enumerate(iris.feature_names):
            st.write(f"{i+1}. {name}")
        st.write("**Clases:**")
        for i, name in enumerate(iris.target_names):
            st.write(f"- {name} (c√≥digo: {i})")
    
    st.write("**Primeras filas del dataset:**")
    st.dataframe(df.head(10))
    
    # Conversi√≥n a DataFrame y agregar nombres de columnas
    st.subheader("2Ô∏è‚É£ Conversi√≥n a DataFrame con Nombres de Columnas")
    
    st.write("‚úÖ El dataset ya est√° convertido a DataFrame con nombres de columnas")
    st.write("**Estructura del DataFrame:**")
    buffer = StringIO()
    df.info(buf=buffer)
    info_str = buffer.getvalue()
    st.text(info_str)
    
    # Estad√≠sticas descriptivas
    st.write("**Estad√≠sticas descriptivas:**")
    st.dataframe(df.describe())
    
    st.write("**Distribuci√≥n de clases:**")
    class_dist = df['target_name'].value_counts()
    st.dataframe(class_dist.to_frame('Cantidad'))
    
    # Estandarizaci√≥n con StandardScaler
    st.subheader("3Ô∏è‚É£ Estandarizaci√≥n con StandardScaler")
    
    # Separar caracter√≠sticas y target
    X = df[iris.feature_names].copy()
    y = df['target'].copy()
    
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X),
        columns=iris.feature_names,
        index=X.index
    )
    
    st.success("‚úÖ Estandarizaci√≥n completada")
    
    col1, col2 = st.columns(2)
    with col1:
        st.write("**Antes de estandarizaci√≥n:**")
        st.dataframe(X.describe())
    
    with col2:
        st.write("**Despu√©s de estandarizaci√≥n:**")
        st.dataframe(X_scaled.describe())
    
    # Divisi√≥n del dataset
    st.subheader("4Ô∏è‚É£ Divisi√≥n del Dataset (70% entrenamiento, 30% prueba)")
    
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42, stratify=y
    )
    
    st.success("‚úÖ Datos divididos en 70% entrenamiento y 30% prueba")
    
    col1, col2 = st.columns(2)
    with col1:
        st.write("**Conjunto de Entrenamiento:**")
        st.write(f"- Filas: {X_train.shape[0]}")
        st.write(f"- Columnas: {X_train.shape[1]}")
        st.write("**Distribuci√≥n de clases:**")
        train_dist = pd.Series(y_train).map({i: name for i, name in enumerate(iris.target_names)}).value_counts()
        st.dataframe(train_dist.to_frame('Cantidad'))
    
    with col2:
        st.write("**Conjunto de Prueba:**")
        st.write(f"- Filas: {X_test.shape[0]}")
        st.write(f"- Columnas: {X_test.shape[1]}")
        st.write("**Distribuci√≥n de clases:**")
        test_dist = pd.Series(y_test).map({i: name for i, name in enumerate(iris.target_names)}).value_counts()
        st.dataframe(test_dist.to_frame('Cantidad'))
    
    # Gr√°fico de dispersi√≥n
    st.subheader("5Ô∏è‚É£ Visualizaci√≥n: Gr√°fico de Dispersi√≥n")
    
    st.markdown("""
    **Gr√°fico de dispersi√≥n: Sepal Length vs Petal Length diferenciado por clase**
    """)
    
    # Crear DataFrame para el gr√°fico
    plot_df = X_scaled.copy()
    plot_df['target'] = df['target'].values
    plot_df['target_name'] = df['target_name'].values
    
    # Crear el gr√°fico
    fig, ax = plt.subplots(figsize=(10, 6))
    
    colors = ['red', 'blue', 'green']
    target_names = iris.target_names
    
    for i, (target, name) in enumerate(zip(range(len(target_names)), target_names)):
        mask = plot_df['target'] == target
        ax.scatter(
            plot_df.loc[mask, 'sepal length (cm)'],
            plot_df.loc[mask, 'petal length (cm)'],
            c=colors[i],
            label=name,
            alpha=0.7,
            s=50
        )
    
    ax.set_xlabel('Sepal Length (cm) [Estandarizado]', fontsize=12)
    ax.set_ylabel('Petal Length (cm) [Estandarizado]', fontsize=12)
    ax.set_title('Distribuci√≥n de Sepal Length vs Petal Length por Clase', fontsize=14, fontweight='bold')
    ax.legend(title='Clase', fontsize=10)
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    st.pyplot(fig)
    
    # Gr√°fico adicional: todas las combinaciones de caracter√≠sticas
    if st.checkbox("Mostrar gr√°ficos adicionales de todas las caracter√≠sticas"):
        st.write("**Matriz de dispersi√≥n de todas las caracter√≠sticas:**")
        
        plot_df_vis = X_scaled.copy()
        plot_df_vis['target_name'] = df['target_name'].values
        
        # sns.pairplot crea su propia figura, as√≠ que no necesitamos crear una antes
        pair_grid = sns.pairplot(plot_df_vis, hue='target_name', diag_kind='hist', palette=['red', 'blue', 'green'], height=2.5)
        pair_grid.fig.suptitle('Matriz de Dispersi√≥n - Todas las Caracter√≠sticas', y=1.02, fontsize=16)
        plt.tight_layout()
        st.pyplot(pair_grid.fig)
    
    # Estad√≠sticas descriptivas del dataset estandarizado
    st.subheader("üìä Salida Esperada")
    
    st.write("**Estad√≠sticas descriptivas del dataset estandarizado:**")
    st.dataframe(X_scaled.describe())
    
    st.write("**Resumen de las operaciones realizadas:**")
    st.code(f"""
Dataset Iris:
- Total de muestras: {df.shape[0]}
- Caracter√≠sticas: {len(iris.feature_names)}
- Clases: {len(iris.target_names)}

Despu√©s de estandarizaci√≥n:
- Media de cada caracter√≠stica: ~0
- Desviaci√≥n est√°ndar de cada caracter√≠stica: ~1

Divisi√≥n de datos:
- X_train: {X_train.shape} ({X_train.shape[0]/len(df)*100:.1f}%)
- X_test: {X_test.shape} ({X_test.shape[0]/len(df)*100:.1f}%)
- y_train: {y_train.shape}
- y_test: {y_test.shape}

Distribuci√≥n balanceada por clase en ambos conjuntos.
    """)
    
    # Informaci√≥n adicional
    st.markdown("---")
    st.info("""
    üí° **Nota:** El dataset Iris es ideal para aprendizaje porque:
    - Es peque√±o y manejable
    - Tiene caracter√≠sticas bien definidas
    - Las clases est√°n balanceadas
    - No contiene valores nulos
    - Es perfecto para visualizaci√≥n
    """)

